{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron\n",
    "\n",
    "<img src=\"image/multilayer.png\" width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeedForward Neural Networks\n",
    "\n",
    ">A feedforward neural network is an artificial neural network where connections between the nodes do not form a cycle.\n",
    "\n",
    "Training this kind of network requires three steps.\n",
    "\n",
    "1. Forward propagation\n",
    "2. Computing cost\n",
    "3. Backpropagation\n",
    "\n",
    "\n",
    " ## 1. Forward Propagation\n",
    "\n",
    "Let `X` be the input vector to the neural network, so that `a[0] = X`.\n",
    "Now, we need to calculate `a[l]` for every layer `l` in the network.\n",
    "Before calculating the activation, `a[l]`, we will calculate an intermediate value `z[l]`. Each element `k` in `z[l]` is just the sum of bias for the neuron `k` in the layer `l` with the weighted sum of the activation of the previous layer, `l-1`.\n",
    "\n",
    "We can calculate `z[l]` from the following equation:\n",
    "\n",
    "<img src=\"image/e1.png\" width='450'>\n",
    "\n",
    "Now that we have `z[l]`, we can compute `a[l]` easily by applying the activation function `g[l]` element-wise to the vector `z[l]`.\n",
    "\n",
    "<img src=\"image/e2.png\" width='300'>\n",
    "\n",
    "this will go for all layers. We can show it like this. \n",
    "\n",
    "<img src=\"image/g1.png\" width='270'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cost Funtion: \n",
    "\n",
    "We will use cross-entropy cost function\n",
    "\n",
    "<img src=\"image/cost.png\" width='300'>\n",
    "\n",
    "Here,\n",
    "\n",
    "$\\hat{y}$ = predicted output\n",
    "\n",
    "$y$ = real output\n",
    "\n",
    "$\\log$  refers natural logarithm ( $\\ln$ )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Backpropagation\n",
    "\n",
    "The goal of backpropagation is to compute the partial derivatives of the `cost function` ($C$) with respect to any `weight` ($w$) or `bias` ($b$) in the network.\n",
    "\n",
    "Once we have these partial derivatives, we will update the weights and biases in the network by the product of some constant `alpha` ( $\\alpha$ ) and the partial derivative of that quantity with respect to the cost function.Here `alpha` is the learning rate which we already used in Xgboost and other algorithms. This way of updating `weight` is known as gradient descent algorithm.\n",
    "\n",
    "<img src=\"image/update.png\" width='300'>\n",
    "\n",
    "Visual of how it works: \n",
    "<img src=\"image/gradient.png\" width='350'>\n",
    "\n",
    "\n",
    "### Chain Rule \n",
    "If $y = f(u)$ ,\n",
    "\n",
    "$u = g(x)$ \n",
    "\n",
    "Both differentiable functions, then\n",
    "\n",
    "$\\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial u} . \\frac{\\partial u}{\\partial x} $\n",
    "\n",
    "**The partial derivative of the cost function `C` with respect to `w[3]`, `b[3]`.\n",
    "Using chain rule:**\n",
    "<img src=\"image/w3.png\" width='350'>\n",
    "\n",
    "Also \n",
    "\n",
    "<img src=\"image/w2.png\" width='350'>\n",
    "\n",
    "And \n",
    "\n",
    "<img src=\"image/w1.png\" width='350'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The partial derivative\n",
    "\n",
    "<img src=\"image/f.png\" width='350'>\n",
    "\n",
    "So for calculating the partial derivatives of C with respect to `w[l]`, `b[l]`, we need to calculate\n",
    "\n",
    "<img src=\"image/f2.png\" width='350'>\n",
    "\n",
    "\n",
    "## For las layer `L`\n",
    "\n",
    "<img src=\"image/zL.png\" width='350'>\n",
    "\n",
    "Where .* represents element-wise multiplication of the matrices, also known as the Hadamard product. We multiply element-wise to make sure that all the dimensions of our matrix multiplications match up as expected.\n",
    "\n",
    "**Derivative of the activation function:**\n",
    "\n",
    "<img src=\"image/a.png\" width='350'>\n",
    "\n",
    "from both :\n",
    "\n",
    "<img src=\"image/az.png\" width='350'>\n",
    "\n",
    "\n",
    "### Partial derivative of C with respect to `z[l]`\n",
    "\n",
    "We want the partial derivative of `C` with respect to `z[l]` in terms of the partial derivative of `C` with respect to the layer `l+1`, so that once we have `z[L]`, we can calculate `z[L-1]`, `z[L-2]`.. and so on.\n",
    "We can express `C` as a function of `z[l + 1]` for any given `l`. \n",
    "\n",
    "Therefore, we can write:\n",
    "\n",
    "<img src=\"image/zl2.png\" width='350'>\n",
    "<img src=\"image/zl3.png\" width='350'>\n",
    "\n",
    "Putting it all together we get:\n",
    "<img src=\"image/zl4.png\" width='350'>\n",
    "\n",
    "Note that we have adjusted the terms to make sure our matrix multiplication dimensions match as expected. Here ‘.’ represents the matrix multiplication operation and .* represents the element-wise product as above.\n",
    "\n",
    "\n",
    "[link] : https://medium.com/binaryandmore/beginners-guide-to-deriving-and-implementing-backpropagation-e3c1a5a1e536#:~:text=The%20goal%20of%20backpropagation%20is,bias%20b%20in%20the%20network.&text=The%20partial%20derivatives%20give%20us%20the%20direction%20of%20greatest%20ascent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self, architecture):\n",
    "        #architecture - numpy array with ith element representing the number of neurons in the ith layer.\n",
    "        \n",
    "        #Initialize the network architecture\n",
    "        self.L = architecture.size - 1 #The index of the last layer L\n",
    "        self.n = architecture #n stores the number of neurons in each layer\n",
    "        self.input_size = self.n[0] #input_size is the number of neurons in the first layer\n",
    "        self.output_size = self.n[self.L] #output_size is the number of neurons in the last layer\n",
    "\n",
    "        #Parameters will store the weights and biases\n",
    "        self.parameters = {}\n",
    "        \n",
    "        #Initialize the network weights and biases:\n",
    "        for i in range (1, self.L + 1): \n",
    "            #Initialize weights to small random values\n",
    "            self.parameters['W' + str(i)] = np.random.randn(self.n[i], self.n[i - 1]) * 0.01\n",
    "            \n",
    "            #Initialize rest of the parameters to 1\n",
    "            self.parameters['b' + str(i)] = np.ones((self.n[i], 1))\n",
    "            self.parameters['z' + str(i)] = np.ones((self.n[i], 1))\n",
    "            self.parameters['a' + str(i)] = np.ones((self.n[i], 1))\n",
    "        print(self.parameters)\n",
    "        #As we started the loop from 1, we haven't initialized a[0]:\n",
    "        self.parameters['a0'] = np.ones((self.n[i], 1))\n",
    "        \n",
    "        #Initialize the cost:\n",
    "        self.parameters['C'] = 1\n",
    "        \n",
    "        #Create a dictionary for storing the derivatives:\n",
    "        self.derivatives = {}\n",
    "        \n",
    "        #Learning rate\n",
    "        self.alpha = 0.01\n",
    "            \n",
    "    def forward_propagate(self, X):\n",
    "        #Note that X here, is just one training example\n",
    "        self.parameters['a0'] = X\n",
    "        \n",
    "        #Calculate the activations for every layer l\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.parameters['z' + str(l)] = np.add(np.dot(self.parameters['W' + str(l)], \n",
    "                                                          self.parameters['a' + str(l - 1)]),    \n",
    "                                                   self.parameters['b' + str(l)])\n",
    "            self.parameters['a' + str(l)] = sigmoid(self.parameters['z' + str(l)])\n",
    "        \n",
    "    def compute_cost(self, y):\n",
    "        self.parameters['C'] = -(y*np.log(self.parameters['a' + str(self.L)]) + \\\n",
    "                                 (1-y)*np.log( 1 - self.parameters['a' + str(self.L)]))\n",
    "    \n",
    "    def compute_derivatives(self, y):\n",
    "        #Partial derivatives of the cost function with respect to z[L], W[L] and b[L]:        \n",
    "        #dzL\n",
    "        self.derivatives['dz' + str(self.L)] = self.parameters['a' + str(self.L)] - y\n",
    "        #dWL\n",
    "        self.derivatives['dW' + str(self.L)] = np.dot(self.derivatives['dz' + str(self.L)], \n",
    "                                                      np.transpose(self.parameters['a' + str(self.L - 1)]))\n",
    "        #dbL\n",
    "        self.derivatives['db' + str(self.L)] = self.derivatives['dz' + str(self.L)]\n",
    "\n",
    "        #Partial derivatives of the cost function with respect to z[l], W[l] and b[l]\n",
    "        for l in range(self.L-1, 0, -1):\n",
    "            self.derivatives['dz' + str(l)] = np.dot(\n",
    "                np.transpose(self.parameters['W' + str(l + 1)]), \n",
    "                self.derivatives['dz' + str(l + 1)])*sigmoid_prime(self.parameters['z' + str(l)])\n",
    "            self.derivatives['dW' + str(l)] = np.dot(\n",
    "                self.derivatives['dz' + str(l)], \n",
    "                np.transpose(self.parameters['a' + str(l - 1)]))\n",
    "            self.derivatives['db' + str(l)] = self.derivatives['dz' + str(l)]\n",
    "            \n",
    "    def update_parameters(self):\n",
    "        for l in range(1, self.L+1):\n",
    "            self.parameters['W' + str(l)] -= self.alpha*self.derivatives['dW' + str(l)]\n",
    "            self.parameters['b' + str(l)] -= self.alpha*self.derivatives['db' + str(l)]\n",
    "        \n",
    "    def predict(self, x):\n",
    "        self.forward_propagate(x)\n",
    "        return self.parameters['a' + str(self.L)]\n",
    "        \n",
    "    def fit(self, X, Y, num_iter):\n",
    "        for iter in range(0, num_iter):\n",
    "            c = 0\n",
    "            acc = 0\n",
    "            n_c = 0\n",
    "            for i in range(0, X.shape[0]):\n",
    "                x = X[i].reshape((X[i].size, 1))\n",
    "                y = Y[i]\n",
    "                self.forward_propagate(x)\n",
    "                self.compute_cost(y)\n",
    "                c += self.parameters['C'] \n",
    "                y_pred = self.predict(x)\n",
    "                y_pred = (y_pred > 0.5)\n",
    "                if y_pred == y:\n",
    "                    n_c += 1\n",
    "                self.compute_derivatives(y)\n",
    "                self.update_parameters()\n",
    "            \n",
    "            c = c/X.shape[0]\n",
    "            acc = (n_c/X.shape[0])*100\n",
    "            print('Iteration: ', iter)\n",
    "            print(\"Cost: \", c)\n",
    "            print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 140 entries, 0 to 139\n",
      "Data columns (total 8 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Area                     140 non-null    float64\n",
      " 1   Perimeter                140 non-null    float64\n",
      " 2   Compactness              140 non-null    float64\n",
      " 3   Length of Kernel         140 non-null    float64\n",
      " 4   Width of Kernel          140 non-null    float64\n",
      " 5   Asymmetry Coefficient    140 non-null    float64\n",
      " 6   Length of Kernel Groove  140 non-null    float64\n",
      " 7   Class                    140 non-null    int64  \n",
      "dtypes: float64(7), int64(1)\n",
      "memory usage: 8.9 KB\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('wheat-seeds-binary.csv')\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "shuffled_dataset['Class'] = shuffled_dataset['Class'] - 1\n",
    "\n",
    "X = shuffled_dataset.iloc[:, 0:-1].values\n",
    "y = shuffled_dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_X = StandardScaler()\n",
    "X = sc_X.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = np.array([7, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 0.00100735,  0.02481865, -0.0116685 ,  0.0212643 , -0.00084768,\n",
      "         0.01209944, -0.0057515 ],\n",
      "       [ 0.00236923, -0.00109906, -0.01647006,  0.00413856, -0.0037008 ,\n",
      "         0.004628  ,  0.0068499 ]]), 'b1': array([[1.],\n",
      "       [1.]]), 'z1': array([[1.],\n",
      "       [1.]]), 'a1': array([[1.],\n",
      "       [1.]]), 'W2': array([[0.00583551, 0.00133127]]), 'b2': array([[1.]]), 'z2': array([[1.]]), 'a2': array([[1.]])}\n"
     ]
    }
   ],
   "source": [
    "classifier = NeuralNetwork(architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Cost:  [[0.75614427]]\n",
      "Accuracy: 53.06122448979592\n",
      "Iteration:  1\n",
      "Cost:  [[0.71851975]]\n",
      "Accuracy: 53.06122448979592\n",
      "Iteration:  2\n",
      "Cost:  [[0.70017197]]\n",
      "Accuracy: 53.06122448979592\n",
      "Iteration:  3\n",
      "Cost:  [[0.68899374]]\n",
      "Accuracy: 53.06122448979592\n",
      "Iteration:  4\n",
      "Cost:  [[0.67987434]]\n",
      "Accuracy: 53.06122448979592\n",
      "Iteration:  5\n",
      "Cost:  [[0.67062775]]\n",
      "Accuracy: 54.08163265306123\n",
      "Iteration:  6\n",
      "Cost:  [[0.66020066]]\n",
      "Accuracy: 68.36734693877551\n",
      "Iteration:  7\n",
      "Cost:  [[0.64801616]]\n",
      "Accuracy: 88.77551020408163\n",
      "Iteration:  8\n",
      "Cost:  [[0.6337837]]\n",
      "Accuracy: 89.79591836734694\n",
      "Iteration:  9\n",
      "Cost:  [[0.61747034]]\n",
      "Accuracy: 88.77551020408163\n",
      "Iteration:  10\n",
      "Cost:  [[0.59929194]]\n",
      "Accuracy: 87.75510204081633\n",
      "Iteration:  11\n",
      "Cost:  [[0.57966485]]\n",
      "Accuracy: 89.79591836734694\n",
      "Iteration:  12\n",
      "Cost:  [[0.55911628]]\n",
      "Accuracy: 89.79591836734694\n",
      "Iteration:  13\n",
      "Cost:  [[0.53818453]]\n",
      "Accuracy: 89.79591836734694\n",
      "Iteration:  14\n",
      "Cost:  [[0.51734208]]\n",
      "Accuracy: 89.79591836734694\n",
      "Iteration:  15\n",
      "Cost:  [[0.49695592]]\n",
      "Accuracy: 89.79591836734694\n",
      "Iteration:  16\n",
      "Cost:  [[0.47728118]]\n",
      "Accuracy: 89.79591836734694\n",
      "Iteration:  17\n",
      "Cost:  [[0.45847504]]\n",
      "Accuracy: 90.81632653061224\n",
      "Iteration:  18\n",
      "Cost:  [[0.44061867]]\n",
      "Accuracy: 91.83673469387756\n",
      "Iteration:  19\n",
      "Cost:  [[0.42373923]]\n",
      "Accuracy: 91.83673469387756\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 1 Got: 0\n",
      "Expected: 0 Got: 0\n",
      "Expected: 1 Got: 1\n",
      "Expected: 0 Got: 0\n",
      "Expected: 1 Got: 0\n",
      "Expected: 1 Got: 1\n",
      "Expected: 1 Got: 1\n",
      "Expected: 0 Got: 0\n",
      "Expected: 1 Got: 1\n",
      "Expected: 1 Got: 1\n",
      "Expected: 1 Got: 1\n",
      "Expected: 0 Got: 0\n",
      "Expected: 0 Got: 0\n",
      "Expected: 0 Got: 0\n",
      "Expected: 0 Got: 0\n",
      "Expected: 1 Got: 1\n",
      "Expected: 1 Got: 1\n",
      "Expected: 0 Got: 0\n",
      "Expected: 1 Got: 0\n",
      "Expected: 1 Got: 1\n",
      "Expected: 0 Got: 0\n",
      "Expected: 1 Got: 0\n",
      "Expected: 0 Got: 0\n",
      "Expected: 1 Got: 1\n",
      "Expected: 0 Got: 0\n",
      "Expected: 0 Got: 0\n",
      "Expected: 0 Got: 0\n",
      "Expected: 1 Got: 1\n",
      "Expected: 0 Got: 0\n",
      "Expected: 1 Got: 1\n",
      "Expected: 1 Got: 1\n",
      "Expected: 0 Got: 0\n",
      "Expected: 1 Got: 0\n",
      "Expected: 0 Got: 0\n",
      "Expected: 1 Got: 1\n",
      "Expected: 0 Got: 0\n",
      "Expected: 0 Got: 0\n",
      "Expected: 0 Got: 0\n",
      "Expected: 1 Got: 1\n",
      "Expected: 1 Got: 1\n",
      "Expected: 0 Got: 0\n",
      "Expected: 1 Got: 1\n",
      "Test Accuracy 88.09523809523809\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "n_c = 0\n",
    "for i in range(0, X_test.shape[0]):\n",
    "    x = X_test[i].reshape((X_test[i].size, 1))\n",
    "    y = y_test[i]\n",
    "    y_pred = classifier.predict(x)\n",
    "    y_pred = (y_pred > 0.5)\n",
    "    print('Expected: %d Got: %d' %(y, y_pred))\n",
    "    if y_pred == y:\n",
    "        n_c += 1\n",
    "\n",
    "acc = (n_c/X_test.shape[0])*100\n",
    "print(\"Test Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
